# 拖拉机 (Tractor/Sheng Ji) RL 项目指南

欢迎来到双生 RL Agent 拖拉机项目。本项目旨在使用深度强化学习（Deep RL）训练一个能够玩四人两副牌拖拉机（又称“升级”、“80分”）的智能 Agent。

## 1. 项目简介

这是一个基于 **PyTorch** 实现的强化学习项目。它构建了一个完整的拖拉机游戏环境，并使用分布式的 Actor-Learner 架构进行自我对弈（Self-Play）训练。

*   **游戏模式**: 4 人局，2 对 2 组队。
*   **核心技术**: CNN 神经网络 + Action Masking + 分布式采样。

## 2. 文件结构说明

| 文件名 | 作用 |
| :--- | :--- |
| `train.py` | **训练入口**。启动 ReplayBuffer、Learner 和多个 Actor 进程开始训练。 |
| `env.py` | **游戏环境**。实现了拖拉机的完整规则（发牌、叫主、反主、出牌、结算）。 |
| `model.py` | **神经网络模型**。定义了用于决策的 CNN 架构。 |
| `actor.py` | **演员进程**。负责加载模型，在环境中对弈，收集数据并发送给 Buffer。 |
| `learner.py` | **学习者进程**。负责从 Buffer 读取数据，更新模型参数。 |
| `wrapper.py` | **特征工程**。将游戏里的卡牌数据转换成神经网络可接受的 Tensor 格式。 |
| `mvGen.py` | **动作生成器**。根据手牌生成所有合法的出牌组合（单张、对子、拖拉机等）。 |
| `replay_buffer.py` | **经验池**。用于存储 Actor 产生的训练数据。 |
| `model_pool.py` | **模型池**。用于在多进程间同步模型版本和参数。 |

## 3. 环境准备与运行

### 依赖库
你需要安装 Python 环境以及以下核心库：
*   `torch` (PyTorch)
*   `numpy`

### 如何开始训练
在终端中运行以下命令：

```bash
python train.py
```

### 运行流程
1.  程序会启动一个 `Learner` 进程和 4 个 `Actor` 进程。
2.  `Actor` 会不断进行自我对弈（4 个位置都由 AI 控制）。
3.  终端会打印每个 Episode 的奖励（Reward）和当前模型版本。
4.  模型断点（Checkpoint）会保存在 `checkpoint/` 目录下（需确保该目录存在）。

## 4. 游戏规则适配
目前的实现涵盖了拖拉机的核心机制：
*   **发牌阶段**: 包含叫主（Report）和反主（Snatch）逻辑。
*   **出牌阶段**: 包含出单张、对子、连对（拖拉机）、甩牌（Throw）以及毙牌（Ruff）检测。
*   **结算**: 每一轮结束计算得分，最后根据闲家得分决定升级情况。

## 5. 常见问题 (FAQ)

**Q: 为什么叫“双生” (Twin)?**
A: 虽然代码中没有显式的 "Twin" 类，但这通常指代 **Actor-Learner** 的双重架构，或者指代拖拉机这种 **2v2 结对** 的游戏性质（Agent 需要学会配合队友）。

**Q: 我该如何修改模型?**
A: 修改 `model.py` 中的 `CNNModel` 类。确保留持 `forward` 函数的输入输出接口不变。

**Q: 训练需要多长时间?**
A: 取决于你的 CPU 核心数（因为是多进程 Actor）。通常需要训练数万个 Episode 才能看到 Agent 学会基本的出牌策略。

